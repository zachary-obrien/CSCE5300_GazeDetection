{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import __main__\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time, math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.autograd.variable import Variable\n",
    "\n",
    "import math, shutil, os, time, argparse, json, re, sys\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "#prepareDatset\n",
    "\n",
    "def readJson(filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        logError('Warning: No such file %s!' % filename)\n",
    "        return None\n",
    "\n",
    "    with open(filename) as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except:\n",
    "            data = None\n",
    "\n",
    "    if data is None:\n",
    "        logError('Warning: Could not read file %s!' % filename)\n",
    "        return None\n",
    "\n",
    "    return data\n",
    "\n",
    "def cropImage(img, bbox):\n",
    "    bbox = np.array(bbox, int)\n",
    "\n",
    "    aSrc = np.maximum(bbox[:2], 0)\n",
    "    bSrc = np.minimum(bbox[:2] + bbox[2:], (img.shape[1], img.shape[0]))\n",
    "\n",
    "    aDst = aSrc - bbox[:2]\n",
    "    bDst = aDst + (bSrc - aSrc)\n",
    "\n",
    "    res = np.zeros((bbox[3], bbox[2], img.shape[2]), img.dtype)    \n",
    "    res[aDst[1]:bDst[1],aDst[0]:bDst[0],:] = img[aSrc[1]:bSrc[1],aSrc[0]:bSrc[0],:]\n",
    "\n",
    "    return res\n",
    "\n",
    "def logError(msg, critical = False):\n",
    "    print(msg)\n",
    "    if critical:\n",
    "        sys.exit(1)\n",
    "\n",
    "# Hard Code this for now using a relative path\n",
    "dataset_path = './100Subsets'  #~#<-- Update this to reflect the correct Folder Structure\n",
    "output_path = './Extracted_Data'   #~#<-- Update this to reflect the correct Folder Structure\n",
    "\n",
    "def preparePath(path, clear = False):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path, 0o777)\n",
    "    if clear:\n",
    "        files = os.listdir(path)\n",
    "        for f in files:\n",
    "            fPath = os.path.join(path, f)\n",
    "            if os.path.isdir(fPath):\n",
    "                shutil.rmtree(fPath)\n",
    "            else:\n",
    "                os.remove(fPath)\n",
    "\n",
    "    return path\n",
    "\n",
    "preparePath(output_path)\n",
    "\n",
    "# list recordings\n",
    "recordings = os.listdir(dataset_path)\n",
    "recordings = np.array(recordings, np.object)\n",
    "recordings = recordings[[os.path.isdir(os.path.join(dataset_path, r)) for r in recordings]]\n",
    "recordings.sort()\n",
    "\n",
    "# Output structure\n",
    "meta = {\n",
    "    'labelRecNum': [],\n",
    "    'frameIndex': [],\n",
    "    'labelDotXCam': [],\n",
    "    'labelDotYCam': [],\n",
    "    'labelFaceGrid': [],\n",
    "}\n",
    "\n",
    "for i,recording in enumerate(recordings):\n",
    "    print('[%d/%d] Processing recording %s (%.2f%%)' % (i, len(recordings), recording, i / len(recordings) * 100))\n",
    "    recDir = os.path.join(dataset_path, recording)\n",
    "    recDirOut = os.path.join(output_path, recording)\n",
    "    \n",
    "            # Read JSONs\n",
    "    appleFace = readJson(os.path.join(recDir, 'appleFace.json'))\n",
    "    if appleFace is None:\n",
    "        continue\n",
    "    appleLeftEye = readJson(os.path.join(recDir, 'appleLeftEye.json'))\n",
    "    if appleLeftEye is None:\n",
    "        continue\n",
    "    appleRightEye = readJson(os.path.join(recDir, 'appleRightEye.json'))\n",
    "    if appleRightEye is None:\n",
    "        continue\n",
    "    dotInfo = readJson(os.path.join(recDir, 'dotInfo.json'))\n",
    "    if dotInfo is None:\n",
    "        continue\n",
    "    faceGrid = readJson(os.path.join(recDir, 'faceGrid.json'))\n",
    "    if faceGrid is None:\n",
    "        continue\n",
    "    frames = readJson(os.path.join(recDir, 'frames.json'))\n",
    "    if frames is None:\n",
    "        continue\n",
    "    # info = readJson(os.path.join(recDir, 'info.json'))\n",
    "    # if info is None:\n",
    "    #     continue\n",
    "    # screen = readJson(os.path.join(recDir, 'screen.json'))\n",
    "    # if screen is None:\n",
    "    #     continue\n",
    "\n",
    "    facePath = preparePath(os.path.join(recDirOut, 'appleFace'))\n",
    "    leftEyePath = preparePath(os.path.join(recDirOut, 'appleLeftEye'))\n",
    "    rightEyePath = preparePath(os.path.join(recDirOut, 'appleRightEye'))\n",
    "\n",
    "    # Preprocess\n",
    "    allValid = np.logical_and(np.logical_and(appleFace['IsValid'], appleLeftEye['IsValid']), np.logical_and(appleRightEye['IsValid'], faceGrid['IsValid']))\n",
    "    if not np.any(allValid):\n",
    "        continue\n",
    "\n",
    "    frames = np.array([int(re.match('(\\d{5})\\.jpg$', x).group(1)) for x in frames])\n",
    "\n",
    "    bboxFromJson = lambda data: np.stack((data['X'], data['Y'], data['W'],data['H']), axis=1).astype(int)\n",
    "    faceBbox = bboxFromJson(appleFace) + [-1,-1,1,1] # for compatibility with matlab code\n",
    "    leftEyeBbox = bboxFromJson(appleLeftEye) + [0,-1,0,0]\n",
    "    rightEyeBbox = bboxFromJson(appleRightEye) + [0,-1,0,0]\n",
    "    leftEyeBbox[:,:2] += faceBbox[:,:2] # relative to face\n",
    "    rightEyeBbox[:,:2] += faceBbox[:,:2]\n",
    "    faceGridBbox = bboxFromJson(faceGrid)\n",
    "    \n",
    "    for j,frame in enumerate(frames):\n",
    "        # Can we use it?\n",
    "        if not allValid[j]:\n",
    "            continue\n",
    "\n",
    "        # Load image\n",
    "        imgFile = os.path.join(recDir, 'frames', '%05d.jpg' % frame)\n",
    "        if not os.path.isfile(imgFile):\n",
    "            logError('Warning: Could not read image file %s!' % imgFile)\n",
    "            continue\n",
    "        img = Image.open(imgFile)        \n",
    "        if img is None:\n",
    "            logError('Warning: Could not read image file %s!' % imgFile)\n",
    "            continue\n",
    "        img = np.array(img.convert('RGB'))\n",
    "\n",
    "        # Crop images\n",
    "        imFace = cropImage(img, faceBbox[j,:])\n",
    "        imEyeL = cropImage(img, leftEyeBbox[j,:])\n",
    "        imEyeR = cropImage(img, rightEyeBbox[j,:])\n",
    "\n",
    "        # Save images\n",
    "        Image.fromarray(imFace).save(os.path.join(facePath, '%05d.jpg' % frame), quality=95)\n",
    "        Image.fromarray(imEyeL).save(os.path.join(leftEyePath, '%05d.jpg' % frame), quality=95)\n",
    "        Image.fromarray(imEyeR).save(os.path.join(rightEyePath, '%05d.jpg' % frame), quality=95)\n",
    "\n",
    "        # Collect metadata\n",
    "        meta['labelRecNum'] += [int(recording)]\n",
    "        meta['frameIndex'] += [frame]\n",
    "        meta['labelDotXCam'] += [dotInfo['XCam'][j]]\n",
    "        meta['labelDotYCam'] += [dotInfo['YCam'][j]]\n",
    "        meta['labelFaceGrid'] += [faceGridBbox[j,:]]\n",
    "        \n",
    "        \n",
    "# Integrate\n",
    "meta['labelRecNum'] = np.stack(meta['labelRecNum'], axis = 0).astype(np.int16)\n",
    "meta['frameIndex'] = np.stack(meta['frameIndex'], axis = 0).astype(np.int32)\n",
    "meta['labelDotXCam'] = np.stack(meta['labelDotXCam'], axis = 0)\n",
    "meta['labelDotYCam'] = np.stack(meta['labelDotYCam'], axis = 0)\n",
    "meta['labelFaceGrid'] = np.stack(meta['labelFaceGrid'], axis = 0).astype(np.uint8)  \n",
    "\n",
    "# Load reference metadata\n",
    "print('Will compare to the reference GitHub dataset metadata.mat...')\n",
    "reference = sio.loadmat('./reference_metadata.mat', struct_as_record=False) \n",
    "reference['labelRecNum'] = reference['labelRecNum'].flatten()\n",
    "reference['frameIndex'] = reference['frameIndex'].flatten()\n",
    "reference['labelDotXCam'] = reference['labelDotXCam'].flatten()\n",
    "reference['labelDotYCam'] = reference['labelDotYCam'].flatten()\n",
    "reference['labelTrain'] = reference['labelTrain'].flatten()\n",
    "reference['labelVal'] = reference['labelVal'].flatten()\n",
    "reference['labelTest'] = reference['labelTest'].flatten()\n",
    "\n",
    "# Find mapping\n",
    "mKey = np.array(['%05d_%05d' % (rec, frame) for rec, frame in zip(meta['labelRecNum'], meta['frameIndex'])], np.object)\n",
    "rKey = np.array(['%05d_%05d' % (rec, frame) for rec, frame in zip(reference['labelRecNum'], reference['frameIndex'])], np.object)\n",
    "mIndex = {k: i for i,k in enumerate(mKey)}\n",
    "rIndex = {k: i for i,k in enumerate(rKey)}\n",
    "mToR = np.zeros((len(mKey,)),int) - 1\n",
    "for i,k in enumerate(mKey):\n",
    "    if k in rIndex:\n",
    "        mToR[i] = rIndex[k]\n",
    "    else:\n",
    "        logError('Did not find rec_frame %s from the new dataset in the reference dataset!' % k)\n",
    "        \n",
    "rToM = np.zeros((len(rKey,)),int) - 1\n",
    "for i,k in enumerate(rKey):\n",
    "    if k in mIndex:\n",
    "        rToM[i] = mIndex[k]\n",
    "    else:\n",
    "        continue\n",
    "        #logError('Did not find rec_frame %s from the reference dataset in the new dataset!' % k, critical = False)\n",
    "        #break\n",
    "        \n",
    "# Copy split from reference\n",
    "meta['labelTrain'] = np.zeros((len(meta['labelRecNum'],)),np.bool)\n",
    "meta['labelVal'] = np.ones((len(meta['labelRecNum'],)),np.bool) # default choice\n",
    "meta['labelTest'] = np.zeros((len(meta['labelRecNum'],)),np.bool)\n",
    "\n",
    "validMappingMask = mToR >= 0\n",
    "meta['labelTrain'][validMappingMask] = reference['labelTrain'][mToR[validMappingMask]]\n",
    "meta['labelVal'][validMappingMask] = reference['labelVal'][mToR[validMappingMask]]\n",
    "meta['labelTest'][validMappingMask] = reference['labelTest'][mToR[validMappingMask]]\n",
    "\n",
    "# Write out metadata\n",
    "metaFile = os.path.join(output_path, 'metadata.mat')\n",
    "print('Writing out the metadata.mat to %s...' % metaFile)\n",
    "sio.savemat(metaFile, meta)\n",
    "\n",
    "# Statistics\n",
    "nMissing = np.sum(rToM < 0)\n",
    "nExtra = np.sum(mToR < 0)\n",
    "totalMatch = len(mKey) == len(rKey) and np.all(np.equal(mKey, rKey))\n",
    "print('======================\\n\\tSummary\\n======================')    \n",
    "print('Total added %d frames from %d recordings.' % (len(meta['frameIndex']), len(np.unique(meta['labelRecNum']))))\n",
    "if nMissing > 0:\n",
    "    print('There are %d frames missing in the new dataset. This may affect the results. Check the log to see which files are missing.' % nMissing)\n",
    "else:\n",
    "    print('There are no missing files.')\n",
    "if nExtra > 0:\n",
    "    print('There are %d extra frames in the new dataset. This is generally ok as they were marked for validation split only.' % nExtra)\n",
    "else:\n",
    "    print('There are no extra files that were not in the reference dataset.')\n",
    "if totalMatch:\n",
    "    print('The new metadata.mat is an exact match to the reference from GitHub (including ordering)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "'''\n",
    "Data loader for the iTracker.\n",
    "Use prepareDataset.py to convert the dataset from http://gazecapture.csail.mit.edu/ to proper format.\n",
    "Author: Petr Kellnhofer ( pkel_lnho (at) gmai_l.com // remove underscores and spaces), 2018. \n",
    "Website: http://gazecapture.csail.mit.edu/\n",
    "Cite:\n",
    "Eye Tracking for Everyone\n",
    "K.Krafka*, A. Khosla*, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik and A. Torralba\n",
    "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016\n",
    "@inproceedings{cvpr2016_gazecapture,\n",
    "Author = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},\n",
    "Title = {Eye Tracking for Everyone},\n",
    "Year = {2016},\n",
    "Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}\n",
    "}\n",
    "'''\n",
    "\n",
    "MEAN_PATH = './'\n",
    "\n",
    "def loadMetadata(filename, silent = False):\n",
    "    try:\n",
    "        # http://stackoverflow.com/questions/6273634/access-array-contents-from-a-mat-file-loaded-using-scipy-io-loadmat-python\n",
    "        if not silent:\n",
    "            print('\\tReading metadata from %s...' % filename)\n",
    "        metadata = sio.loadmat(filename, squeeze_me=True, struct_as_record=False)\n",
    "    except:\n",
    "        print('\\tFailed to read the meta file \"%s\"!' % filename)\n",
    "        return None\n",
    "    return metadata\n",
    "\n",
    "class SubtractMean(object):\n",
    "    \"\"\"Normalize an tensor image with mean.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, meanImg):\n",
    "        self.meanImg = transforms.ToTensor()(meanImg / 255)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"       \n",
    "        return tensor.sub(self.meanImg)\n",
    "\n",
    "\n",
    "class ITrackerData(data.Dataset):\n",
    "    def __init__(self, dataPath, split = 'train', imSize=(224,224), gridSize=(25, 25)):\n",
    "\n",
    "        self.dataPath = dataPath\n",
    "        self.imSize = imSize\n",
    "        self.gridSize = gridSize\n",
    "\n",
    "        print('Loading iTracker dataset...')\n",
    "        metaFile = os.path.join(dataPath, 'metadata.mat')\n",
    "        #metaFile = 'metadata.mat'\n",
    "        if metaFile is None or not os.path.isfile(metaFile):\n",
    "            raise RuntimeError('There is no such file %s! Provide a valid dataset path.' % metaFile)\n",
    "        self.metadata = loadMetadata(metaFile)\n",
    "        if self.metadata is None:\n",
    "            raise RuntimeError('Could not read metadata file %s! Provide a valid dataset path.' % metaFile)\n",
    "\n",
    "        self.faceMean = loadMetadata(os.path.join(MEAN_PATH, 'mean_face_224.mat'))['image_mean']\n",
    "        self.eyeLeftMean = loadMetadata(os.path.join(MEAN_PATH, 'mean_left_224.mat'))['image_mean']\n",
    "        self.eyeRightMean = loadMetadata(os.path.join(MEAN_PATH, 'mean_right_224.mat'))['image_mean']\n",
    "        \n",
    "        self.transformFace = transforms.Compose([\n",
    "            transforms.Resize(self.imSize),\n",
    "            transforms.ToTensor(),\n",
    "            SubtractMean(meanImg=self.faceMean),\n",
    "        ])\n",
    "        self.transformEyeL = transforms.Compose([\n",
    "            transforms.Resize(self.imSize),\n",
    "            transforms.ToTensor(),\n",
    "            SubtractMean(meanImg=self.eyeLeftMean),\n",
    "        ])\n",
    "        self.transformEyeR = transforms.Compose([\n",
    "            transforms.Resize(self.imSize),\n",
    "            transforms.ToTensor(),\n",
    "            SubtractMean(meanImg=self.eyeRightMean),\n",
    "        ])\n",
    "\n",
    "\n",
    "        if split == 'test':\n",
    "            mask = self.metadata['labelTest']\n",
    "        elif split == 'val':\n",
    "            mask = self.metadata['labelVal']\n",
    "        else:\n",
    "            mask = self.metadata['labelTrain']\n",
    "\n",
    "        self.indices = np.argwhere(mask)[:,0]\n",
    "        print('Loaded iTracker dataset split \"%s\" with %d records...' % (split, len(self.indices)))\n",
    "\n",
    "    def loadImage(self, path):\n",
    "        try:\n",
    "            im = Image.open(path).convert('RGB')\n",
    "        except OSError:\n",
    "            raise RuntimeError('Could not read image: ' + path)\n",
    "            #im = Image.new(\"RGB\", self.imSize, \"white\")\n",
    "\n",
    "        return im\n",
    "\n",
    "\n",
    "    def makeGrid(self, params):\n",
    "        gridLen = self.gridSize[0] * self.gridSize[1]\n",
    "        grid = np.zeros([gridLen,], np.float32)\n",
    "        \n",
    "        indsY = np.array([i // self.gridSize[0] for i in range(gridLen)])\n",
    "        indsX = np.array([i % self.gridSize[0] for i in range(gridLen)])\n",
    "        condX = np.logical_and(indsX >= params[0], indsX < params[0] + params[2]) \n",
    "        condY = np.logical_and(indsY >= params[1], indsY < params[1] + params[3]) \n",
    "        cond = np.logical_and(condX, condY)\n",
    "\n",
    "        grid[cond] = 1\n",
    "        return grid\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.indices[index]\n",
    "\n",
    "        imFacePath = os.path.join(self.dataPath, '%05d/appleFace/%05d.jpg' % (self.metadata['labelRecNum'][index], self.metadata['frameIndex'][index]))\n",
    "        imEyeLPath = os.path.join(self.dataPath, '%05d/appleLeftEye/%05d.jpg' % (self.metadata['labelRecNum'][index], self.metadata['frameIndex'][index]))\n",
    "        imEyeRPath = os.path.join(self.dataPath, '%05d/appleRightEye/%05d.jpg' % (self.metadata['labelRecNum'][index], self.metadata['frameIndex'][index]))\n",
    "\n",
    "        imFace = self.loadImage(imFacePath)\n",
    "        imEyeL = self.loadImage(imEyeLPath)\n",
    "        imEyeR = self.loadImage(imEyeRPath)\n",
    "\n",
    "        imFace = self.transformFace(imFace)\n",
    "        imEyeL = self.transformEyeL(imEyeL)\n",
    "        imEyeR = self.transformEyeR(imEyeR)\n",
    "\n",
    "        gaze = np.array([self.metadata['labelDotXCam'][index], self.metadata['labelDotYCam'][index]], np.float32)\n",
    "\n",
    "        faceGrid = self.makeGrid(self.metadata['labelFaceGrid'][index,:])\n",
    "\n",
    "        # to tensor\n",
    "        row = torch.LongTensor([int(index)])\n",
    "        faceGrid = torch.FloatTensor(faceGrid)\n",
    "        gaze = torch.FloatTensor(gaze)\n",
    "\n",
    "        return row, imFace, imEyeL, imEyeR, faceGrid, gaze\n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time, math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.autograd.variable import Variable\n",
    "\n",
    "'''\n",
    "Pytorch model for the iTracker.\n",
    "Author: Petr Kellnhofer ( pkel_lnho (at) gmai_l.com // remove underscores and spaces), 2018. \n",
    "Website: http://gazecapture.csail.mit.edu/\n",
    "Cite:\n",
    "Eye Tracking for Everyone\n",
    "K.Krafka*, A. Khosla*, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik and A. Torralba\n",
    "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016\n",
    "@inproceedings{cvpr2016_gazecapture,\n",
    "Author = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},\n",
    "Title = {Eye Tracking for Everyone},\n",
    "Year = {2016},\n",
    "Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}\n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "class ItrackerImageModel(nn.Module):\n",
    "    # Used for both eyes (with shared weights) and the face (with unqiue weights)\n",
    "    def __init__(self):\n",
    "        super(ItrackerImageModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.CrossMapLRN2d(size=5, alpha=0.0001, beta=0.75, k=1.0),\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2, groups=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.CrossMapLRN2d(size=5, alpha=0.0001, beta=0.75, k=1.0),\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 64, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "class FaceImageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FaceImageModel, self).__init__()\n",
    "        self.conv = ItrackerImageModel()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(12*12*64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class FaceGridModel(nn.Module):\n",
    "    # Model for the face grid pathway\n",
    "    def __init__(self, gridSize = 25):\n",
    "        super(FaceGridModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(gridSize * gridSize, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class ITrackerModel(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ITrackerModel, self).__init__()\n",
    "        self.eyeModel = ItrackerImageModel()\n",
    "        self.faceModel = FaceImageModel()\n",
    "        self.gridModel = FaceGridModel()\n",
    "        # Joining both eyes\n",
    "        self.eyesFC = nn.Sequential(\n",
    "            nn.Linear(2*12*12*64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "        # Joining everything\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128+64+128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 2),\n",
    "            )\n",
    "\n",
    "    def forward(self, faces, eyesLeft, eyesRight, faceGrids):\n",
    "        # Eye nets\n",
    "        xEyeL = self.eyeModel(eyesLeft)\n",
    "        xEyeR = self.eyeModel(eyesRight)\n",
    "        # Cat and FC\n",
    "        xEyes = torch.cat((xEyeL, xEyeR), 1)\n",
    "        xEyes = self.eyesFC(xEyes)\n",
    "\n",
    "        # Face net\n",
    "        xFace = self.faceModel(faces)\n",
    "        xGrid = self.gridModel(faceGrids)\n",
    "\n",
    "        # Cat all\n",
    "        x = torch.cat((xEyes, xFace, xGrid), 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceImageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FaceImageModel, self).__init__()\n",
    "        self.conv = ItrackerImageModel()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(12*12*64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class FaceGridModel(nn.Module):\n",
    "    # Model for the face grid pathway\n",
    "    def __init__(self, gridSize = 25):\n",
    "        super(FaceGridModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(gridSize * gridSize, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox(frame, x,y,w,h):\n",
    "    BBOX = (\"Box {0}: ({1},{2}), ({3},{4}), ({5},{6}), ({7},{8})\".format(frame,x,y,x+w,y,x+w,y+h,x,y+h))\n",
    "    \n",
    "    return BBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascPath = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "\n",
    "# Create the haar cascade\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Face Detector\n",
    "\n",
    "for Folder in os.listdir('100Subsets'):  #[0:1]:\n",
    "    \n",
    "    imageFolder = r'100Subsets\\\\' + Folder + '\\\\frames\\\\'\n",
    "    \n",
    "    frames = os.listdir(imageFolder)\n",
    "    \n",
    "    for frame in frames:\n",
    "\n",
    "        imagePath = imageFolder + frame\n",
    "\n",
    "        image = cv2.imread(imagePath)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the image\n",
    "        faces = faceCascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "            )\n",
    "        \n",
    "        #print(\"Found {0} faces!\".format(len(faces)))\n",
    "        \n",
    "        # Draw a rectangle around the faces\n",
    "        for (x1, y1, w, h) in faces:\n",
    "            copy = image.copy()\n",
    "            cv2.rectangle(copy, (x1, y1), (x1+w, y1+h), (0, 255, 0), 2)\n",
    "            #cv2.imshow(\"Faces found\", copy)\n",
    "            __main__.__dict__['Bounding_Box'] = bbox(frame, x1,y1,w,h)\n",
    "\n",
    "            print(__main__.__dict__['Bounding_Box'])\n",
    "            \n",
    "            cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change there flags to control what happens.\n",
    "doLoad = 1    # not args.reset # Load checkpoint at the beginning\n",
    "doTest = 0   # args.sink # Only run test, no training\n",
    "\n",
    "workers = 16\n",
    "epochs = 25\n",
    "batch_size = 5\n",
    "\n",
    "print(batch_size)\n",
    "\n",
    "base_lr = 0.0001\n",
    "\n",
    "base_lr = 0.0001\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "print_freq = 10\n",
    "prec1 = 0\n",
    "best_prec1 = 1e20\n",
    "lr = base_lr\n",
    "\n",
    "count_test = 0\n",
    "count = 0\n",
    "\n",
    "def main():\n",
    "    global args, best_prec1, weight_decay, momentum\n",
    "\n",
    "    model = ITrackerModel()\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    #model.cuda()\n",
    "    imSize=(224,224)\n",
    "    cudnn.benchmark = True   \n",
    "\n",
    "    epoch = 0\n",
    "    if doLoad:\n",
    "        saved = load_checkpoint()\n",
    "        if saved:\n",
    "            print('Loading checkpoint for epoch %05d with loss %.5f (which is the mean squared error not the actual linear error)...' % (saved['epoch'], saved['best_prec1']))\n",
    "            state = saved['state_dict']\n",
    "            try:\n",
    "                model.module.load_state_dict(state)\n",
    "            except:\n",
    "                model.load_state_dict(state)\n",
    "            epoch = saved['epoch']\n",
    "            best_prec1 = saved['best_prec1']\n",
    "        else:\n",
    "            print('Warning: Could not read checkpoint!')\n",
    "\n",
    "    \n",
    "    dataTrain = ITrackerData(dataPath = output_path, split='train', imSize = imSize)\n",
    "    dataVal = ITrackerData(dataPath = output_path, split='test', imSize = imSize)\n",
    "   \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataTrain,\n",
    "        batch_size=batch_size, shuffle=True,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        dataVal,\n",
    "        batch_size=batch_size, shuffle=False,\n",
    "        num_workers=workers, pin_memory=True)\n",
    "\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "\n",
    "    # Quick test\n",
    "    if doTest:\n",
    "        validate(val_loader, model, criterion, epoch)\n",
    "        return\n",
    "\n",
    "    for epoch in range(0, epoch):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "    for epoch in range(epoch, epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion, epoch)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 < best_prec1\n",
    "        best_prec1 = min(prec1, best_prec1)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "        }, is_best)\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion,optimizer, epoch):\n",
    "    global count\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for i, (row, imFace, imEyeL, imEyeR, faceGrid, gaze) in enumerate(train_loader):\n",
    "        \n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        imFace = imFace\n",
    "        imEyeL = imEyeL\n",
    "        imEyeR = imEyeR\n",
    "        faceGrid = faceGrid\n",
    "        gaze = gaze\n",
    "        \n",
    "        imFace = torch.autograd.Variable(imFace, requires_grad = True)\n",
    "        imEyeL = torch.autograd.Variable(imEyeL, requires_grad = True)\n",
    "        imEyeR = torch.autograd.Variable(imEyeR, requires_grad = True)\n",
    "        faceGrid = torch.autograd.Variable(faceGrid, requires_grad = True)\n",
    "        gaze = torch.autograd.Variable(gaze, requires_grad = False)\n",
    "\n",
    "        # compute output\n",
    "        output = model(imFace, imEyeL, imEyeR, faceGrid)\n",
    "\n",
    "        loss = criterion(output, gaze)\n",
    "        \n",
    "        losses.update(loss.data.item(), imFace.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        count=count+1\n",
    "\n",
    "        print('Epoch (train): [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch):\n",
    "    global count_test\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    lossesLin = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    oIndex = 0\n",
    "    for i, (row, imFace, imEyeL, imEyeR, faceGrid, gaze) in enumerate(val_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        imFace = imFace\n",
    "        imEyeL = imEyeL\n",
    "        imEyeR = imEyeR\n",
    "        faceGrid = faceGrid\n",
    "        gaze = gaze\n",
    "        \n",
    "        imFace = torch.autograd.Variable(imFace, requires_grad = False)\n",
    "        imEyeL = torch.autograd.Variable(imEyeL, requires_grad = False)\n",
    "        imEyeR = torch.autograd.Variable(imEyeR, requires_grad = False)\n",
    "        faceGrid = torch.autograd.Variable(faceGrid, requires_grad = False)\n",
    "        gaze = torch.autograd.Variable(gaze, requires_grad = False)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(imFace, imEyeL, imEyeR, faceGrid)\n",
    "\n",
    "        loss = criterion(output, gaze)\n",
    "        \n",
    "        lossLin = output - gaze\n",
    "        lossLin = torch.mul(lossLin,lossLin)\n",
    "        lossLin = torch.sum(lossLin,1)\n",
    "        lossLin = torch.mean(torch.sqrt(lossLin))\n",
    "\n",
    "        losses.update(loss.data.item(), imFace.size(0))\n",
    "        lossesLin.update(lossLin.item(), imFace.size(0))\n",
    "     \n",
    "        # compute gradient and do SGD step\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        print('Epoch (val): [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Error L2 {lossLin.val:.4f} ({lossLin.avg:.4f})\\t'.format(\n",
    "                    epoch, i, len(val_loader), batch_time=batch_time,\n",
    "                   loss=losses,lossLin=lossesLin))\n",
    "\n",
    "    return lossesLin.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS_PATH = '.'\n",
    "\n",
    "def load_checkpoint(filename='checkpoint.pth.tar'):\n",
    "    filename = os.path.join(CHECKPOINTS_PATH, filename)\n",
    "    print(filename)\n",
    "    if not os.path.isfile(filename):\n",
    "        return None\n",
    "    state = torch.load(filename)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    if not os.path.isdir(CHECKPOINTS_PATH):\n",
    "        os.makedirs(CHECKPOINTS_PATH, 0o777)\n",
    "    bestFilename = os.path.join(CHECKPOINTS_PATH, 'best_' + filename)\n",
    "    filename = os.path.join(CHECKPOINTS_PATH, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, bestFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = base_lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.state_dict()['param_groups']:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1da653b9d4e8419f9e4e7f34517b2d166406bf3bd28c5de51c8f75fec185e0c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
